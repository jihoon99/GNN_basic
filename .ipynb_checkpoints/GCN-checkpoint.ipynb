{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "authorized-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "italian-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.Crippen import MolLogP\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#from utils import read_ZINC_smiles, smiles_to_onehot, partition, OneHotLogPDataSet\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-egyptian",
   "metadata": {},
   "outputs": [],
   "source": [
    "paser = argparse.ArgumentParser()\n",
    "args = paser.parse_args(\"\")\n",
    "args.seed = 123\n",
    "args.val_size = 0.1\n",
    "args.test_size = 0.1\n",
    "args.shuffle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ordinary-yacht",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-right",
   "metadata": {},
   "source": [
    "# 1. Pre-Processing\n",
    "\n",
    "ZINC.smiles 파일에 text로 표현되어 있는 분자들을 molecular graph 형태로 바꿔줍시다. 이때 node feature matrix는 아래 그림과 같이 각 원자의 symbol, degree 등 화학적 특성을 one-hot vector로 나타낸 형태입니다.\\\n",
    "![node feature matrix](https://github.com/SeungsuKim/CH485--AI-and-Chemistry/raw/c85ce8716ac2e351d730543a2d45fd7054014d4f/Assignments/5.%20GCN/Graph_Generating_Process.png)\n",
    "\n",
    "`read_ZINC_smiles` 함수는 smiles 파일 내의 분자 텍스트의 list와 각 분자들의 실제 logP value list를 return합니다.\n",
    "`convert_to_graph` 함수는 분자 텍스트의 list를 받아 각 분자들의 `node feature matrix list`와 `adjacency matrix list`를 return 합니다.\\\n",
    "분자마다 원소의 크기가 다르므로 max node 수를 정해놓고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "seasonal-springfield",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' ', 'c', 'C', '(', ')', 'O', '1', '2', 'N', '=', '[', ']', '@',\n",
       "       '3', 'H', 'n', '4', 'F', '+', 'S', 'l', 's', '/', 'o', '-', '5',\n",
       "       '#', 'B', 'r', '\\\\', '6', 'I'], dtype='<U1')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(\"ignore/GCN_data/vocab.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "middle-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ZINC_smiles(file_name, num_mol):\n",
    "    '''파일 읽어오고, SMILE과, logP형래토 리턴한다.\n",
    "        file_name : PATH\n",
    "        num_mol : atoms limit\n",
    "    '''\n",
    "    f = open(file_name, 'r')\n",
    "    contents = f.readlines()\n",
    "\n",
    "    smi_list = []\n",
    "    logP_list = []\n",
    "\n",
    "    for i in tqdm_notebook(range(num_mol), desc='Reading Data'):\n",
    "        smi = contents[i].strip() # readlines에서 SMILES를 불러와서\n",
    "        m = Chem.MolFromSmiles(smi) # 변환 for LogP\n",
    "        smi_list.append(smi)\n",
    "        logP_list.append(MolLogP(m)) # \n",
    "\n",
    "    logP_list = np.asarray(logP_list).astype(float)\n",
    "\n",
    "    return smi_list, logP_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def smiles_to_onehot(smi_list):\n",
    "    def smiles_to_vector(smiles, vocab, max_length):\n",
    "        while len(smiles) < max_length:\n",
    "            smiles += \" \"\n",
    "        vector = [vocab.index(str(x)) for x in smiles]\n",
    "        one_hot = np.zeros((len(vocab), max_length), dtype=int)\n",
    "        for i, elm in enumerate(vector):\n",
    "            one_hot[elm][i] = 1\n",
    "        return one_hot\n",
    "\n",
    "    vocab = np.load('ignore/GCN_data/vocab.npy/vocab.npy')\n",
    "    smi_total = []\n",
    "\n",
    "    for i, smi in tqdm_notebook(enumerate(smi_list), desc='Converting to One Hot'):\n",
    "        smi_onehot = smiles_to_vector(smi, list(vocab), 120)\n",
    "        smi_total.append(smi_onehot)\n",
    "\n",
    "    return np.asarray(smi_total)\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_graph(smiles_list):\n",
    "    adj = []\n",
    "    adj_norm = []\n",
    "    features = []\n",
    "    maxNumAtoms = 50\n",
    "\n",
    "    for i in tqdm_notebook(smiles_list, desc='Converting to Graph'):\n",
    "        # Mol\n",
    "        iMol = Chem.MolFromSmiles(i.strip())\n",
    "        #Adj\n",
    "        iAdjTmp = Chem.rdmolops.GetAdjacencyMatrix(iMol) # adjac Matrix\n",
    "        # Feature\n",
    "        if( iAdjTmp.shape[0] <= maxNumAtoms): # 50개의 원자만 할것.\n",
    "            # Feature-preprocessing\n",
    "            iFeature = np.zeros((maxNumAtoms, 58)) # 50*58\n",
    "\n",
    "            # 여기다가 실제 원소 값이 있는 친구들을 넣어.\n",
    "            iFeatureTmp = []\n",
    "            for atom in iMol.GetAtoms():\n",
    "                iFeatureTmp.append( atom_feature(atom) ) ### atom features only : 58 features\n",
    "            iFeature[0:len(iFeatureTmp), 0:58] = iFeatureTmp ### 0 padding for feature-set\n",
    "            features.append(iFeature)\n",
    "\n",
    "            # Adj-preprocessing\n",
    "            iAdj = np.zeros((maxNumAtoms, maxNumAtoms))\n",
    "            iAdj[0:len(iFeatureTmp), 0:len(iFeatureTmp)] = iAdjTmp + np.eye(len(iFeatureTmp)) # eye는 정방행렬 만들려고,, iAdjTmp는 adjcency\n",
    "            adj.append(np.asarray(iAdj))\n",
    "    features = np.asarray(features)\n",
    "\n",
    "    return features, adj\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def atom_feature(atom):\n",
    "    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),\n",
    "                                      ['C', 'N', 'O', 'S', 'F', 'H', 'Si', 'P', 'Cl', 'Br',\n",
    "                                       'Li', 'Na', 'K', 'Mg', 'Ca', 'Fe', 'As', 'Al', 'I', 'B',\n",
    "                                       'V', 'Tl', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn',\n",
    "                                       'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'Mn', 'Cr', 'Pt', 'Hg', 'Pb']) +\n",
    "                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5]) +\n",
    "                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4]) +\n",
    "                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5]) +\n",
    "                    [atom.GetIsAromatic()])    # (40, 6, 5, 6, 1)\n",
    "\n",
    "def one_of_k_encoding(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def one_of_k_encoding_unk(x, allowable_set):\n",
    "    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return list(map(lambda s: x == s, allowable_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "loaded-guidance",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/2560548047.py:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm_notebook(range(num_mol), desc='Reading Data'):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa848ef58fa64d1faf316ffa7694c757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading Data:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/2560548047.py:52: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm_notebook(smiles_list, desc='Converting to Graph'):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fceeb22292bb49a0a999920a5cfc1556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting to Graph:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_smi, list_logP = read_ZINC_smiles('ignore/GCN_data/ZINC.smiles', 100) # 10000개의 atom만 쓰겠다.\n",
    "list_feature, list_adj = convert_to_graph(list_smi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-jefferson",
   "metadata": {},
   "source": [
    "위 코드를 통해 원하는 수 만큼의 분자들의 node feature matrix list인 `list_feature`, adjacency matrix list인 `list_adj`, 그리고 logP value list인 `list_logP`를 얻었습니다.\n",
    "\n",
    "그 동안의 실습에서는 이미지로부터 이미지의 label을 얻는, 즉 하나의 input에서 하나의 output을 얻는 형태였지만, 이번 실습에서는 두 개의 input, `list_feature`와 `list_adj`로부터 logP value라는 하나의 output을 얻어내야합니다. 이를 위해 custom pytorch dataset을 정의하고 사용해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "mineral-traveler",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNDataset(Dataset):\n",
    "    def __init__(self, list_feature, list_adj, list_logP):\n",
    "        self.list_feature = list_feature\n",
    "        self.list_adj = list_adj\n",
    "        self.list_logP = list_logP\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_feature)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''데이터 가져오기 위함'''\n",
    "        return self.list_feature[index], self.list_adj[index], self.list_logP[index]\n",
    "\n",
    "\n",
    "def partition(list_feature, list_adj, list_logP, args):\n",
    "    '''train, valid, test split'''\n",
    "    num_total = list_feature.shape[0]\n",
    "    num_train = int(num_total * (1 - args.test_size - args.val_size))\n",
    "    num_val = int(num_total * args.val_size)\n",
    "    num_test = int(num_total * args.test_size)\n",
    "\n",
    "    feature_train = list_feature[:num_train]\n",
    "    adj_train = list_adj[:num_train]\n",
    "    logP_train = list_logP[:num_train]\n",
    "    \n",
    "    feature_val = list_feature[num_train:num_train + num_val]\n",
    "    adj_val = list_adj[num_train:num_train + num_val]\n",
    "    logP_val = list_logP[num_train:num_train + num_val]\n",
    "    \n",
    "    feature_test = list_feature[num_total - num_test:]\n",
    "    adj_test = list_adj[num_total - num_test:]\n",
    "    logP_test = list_logP[num_total - num_test:]\n",
    "        \n",
    "    train_set = GCNDataset(feature_train, adj_train, logP_train)\n",
    "    val_set = GCNDataset(feature_val, adj_val, logP_val)\n",
    "    test_set = GCNDataset(feature_test, adj_test, logP_test)\n",
    "\n",
    "    partition = {\n",
    "        'train': train_set,\n",
    "        'val': val_set,\n",
    "        'test': test_set\n",
    "    }\n",
    "\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "isolated-envelope",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = partition(list_feature, list_adj, list_logP, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "annoying-hawaiian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <__main__.GCNDataset at 0x1687660d0>,\n",
       " 'val': <__main__.GCNDataset at 0x29ccb5910>,\n",
       " 'test': <__main__.GCNDataset at 0x29ccb57f0>}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-scholarship",
   "metadata": {},
   "source": [
    "# 2. Model Construction\n",
    "\n",
    "Graph Convolution Network, 즉 GCN을 pytorch를 이용하여 구현하여봅시다. 이를 위해 다음과 같은 sub module들을 구현하고 사용합니다.\n",
    "\n",
    "- **GCNLayer**: node feature matrix와 adjacency matrix의 list를 받아 graph convolution 연산을 수행하는 module 입니다.\n",
    "- **(Gated)SkipConnection**: ResNet에서 사용되었던 skip connection technique을 구현한 module 입니다.\n",
    "- **GCNBlock**: node feature matrix와 adjacency matrix의 list를 받아 원하는 갯수의 GCNLayer를 통과시킨 후, (gated)skip connection을 적용하는 module 입니다.\n",
    "- **ReadOut**: graph structrure에 permutation invariance를 주기 위하여 linear layer를 거친 뒤 batch 별로 summation하는 module 입니다.\n",
    "- **Predictor**: ReadOut layer로부터의 graph feature vector로부터 logP value를 예측하기 위한 linear layer module 입니다.\n",
    "\n",
    "위 모듈들을 사용하여 **GCNNet**을 구현해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-stereo",
   "metadata": {},
   "source": [
    "A __GCNNet__ model is composed with __GCNBlocks__, __ReadOut__, and __Predictors__. A __GCNBlock__ is composed with several __GCNLayers__, which implements the basic feature of Graph Convolution Network's layer. A user can choose whether or not to use __batch normalization__ and __attention__ in each __GCNLayer__ and to use __(gated) skip connection__ in each __GCNBlock__. A __ReadOut__ and __Predictor__ are just the fully connected layer which converts the dimension of the layer.\n",
    "\n",
    "__Gated Skip Connection__ and __Attention__ was implemented by by evaluating each coefficient by following equations:\n",
    "\n",
    "$$z_i = \\sigma(U_{z,1}H^{(l+1)}_i + b_{z,1} + U_{z,2}H^{(l)}_i + b_{z,2})$$\n",
    "\n",
    "$$\\alpha_{i,j} = \\sigma((H_iW)C(H_jW)^T)$$\n",
    "\n",
    "Also, __attention__ was implemented as multi-head attention, which learn multi attention matrices that generate a part of output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "powerful-favorite",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, n_atom, act=None, bn=False, atn=False, num_head=1, dropout=0):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        \n",
    "        self.use_bn = bn # batch_normalization\n",
    "        self.use_atn = atn\n",
    "        self.linear = nn.Linear(in_dim, out_dim) # [batch, len(atom), output_dim] \n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        self.bn = nn.BatchNorm1d(n_atom)\n",
    "        self.attention = Attention(out_dim, out_dim, num_head)\n",
    "        self.activation = act\n",
    "        self.dropout_rate = dropout\n",
    "        self.dropout = nn.Dropout2d(self.dropout_rate)\n",
    "        \n",
    "    def forward(self, x, adj):  # featur, adj두개를 받아야함.\n",
    "        out = self.linear(x) # H*W + bias\n",
    "        if self.use_atn:\n",
    "            out = self.attention(out, adj)\n",
    "        else: # A*H*W + bias\n",
    "            out = torch.matmul(adj, out)\n",
    "            \n",
    "        if self.use_bn:\n",
    "            out = self.bn(out)\n",
    "        if self.activation != None:\n",
    "            out = self.activation(out)\n",
    "        if self.dropout_rate > 0:\n",
    "            out = self.dropout(out)\n",
    "        return out, adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "computational-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipConnection(nn.Module):\n",
    "    '''단순히 더해주는 것인데, \n",
    "    사이즈가 다를 때 맞춰줘야한다.'''\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(SkipConnection, self).__init__()\n",
    "        \n",
    "        '''\n",
    "        out_dim -> node_feature matrix의 column갯수\n",
    "        '''\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.linear = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        \n",
    "    def forward(self, in_x, out_x):\n",
    "        if (self.in_dim != self.out_dim):\n",
    "            in_x = self.linear(in_x)\n",
    "        out = in_x + out_x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "sixth-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedSkipConnection(nn.Module):\n",
    "    '''\n",
    "    z와 (1-z)로 섞어서 SkipConnection을 하겠다.\n",
    "    z는 learnable\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GatedSkipConnection, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        # 사이즈가 다르면 맞춰줘야 하니까.\n",
    "        self.linear = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        \n",
    "        self.linear_coef_in = nn.Linear(out_dim, out_dim)\n",
    "        self.linear_coef_out = nn.Linear(out_dim, out_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, in_x, out_x):\n",
    "        # 사이즈가 다르면 맞춰준다.\n",
    "        if (self.in_dim != self.out_dim):\n",
    "            in_x = self.linear(in_x)\n",
    "            \n",
    "        z = self.gate_coefficient(in_x, out_x) # z는 상수야. 그래서 밑에보면 matmul이 아닌 mul을 사용\n",
    "        out = torch.mul(z, out_x) + torch.mul(1.0-z, in_x)\n",
    "        return out\n",
    "            \n",
    "    def gate_coefficient(self, in_x, out_x):\n",
    "        x1 = self.linear_coef_in(in_x)\n",
    "        x2 = self.linear_coef_out(out_x)\n",
    "        return self.sigmoid(x1+x2) # 0~1사이값이 나와야함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1605e4-475a-4659-993f-caeb59316e5f",
   "metadata": {},
   "source": [
    "\n",
    "$$\\alpha_{i,j} = \\sigma((H_iW)C(H_jW)^T)$$\n",
    "\n",
    "$$H_iW -> [num atoms, multi\\ head\\ features]$$\n",
    "$$H_jW -> [multi\\ head\\ features, num\\ atoms]$$\n",
    "$$C -> [multi\\ head\\ features, multi\\ head\\ features]$$\n",
    "i번째 노드와 j번째 노드간의 관계가 얼마나 중요한가를 나타내는 식(?)이다. 논문에 의하면 C는 learnable param인데 두 사이의 관계가 얼마나 관계가 있는지 학습하는 파라미터라고 한다.\n",
    "Also, __attention__ was implemented as multi-head attention, which learn multi attention matrices that generate a part of output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "orange-arthur",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''해석 필요'''\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, output_dim, num_head):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.num_head = num_head\n",
    "        self.atn_dim = output_dim // num_head # 내가 아는 self attention이랑은 다르네.. 음.. output에 꾸겨 맞춘 멀티헤드네..\n",
    "        \n",
    "        # correlation들은 learnable param인데, 여기밑에서는 그냥 텐서를 만들었어..\n",
    "        # modulelist를 만든 이유는, 옵티마이저가 모듈리스트의 안에잇는 파라미터를 보게 하기 위함임.\n",
    "        self.linears = nn.ModuleList() # linear라는 곳에 num head계산 결과들을 저장해두자.\n",
    "        self.corelations = nn.ParameterList()\n",
    "        for i in range(self.num_head):\n",
    "            self.linears.append(nn.Linear(in_dim, self.atn_dim))\n",
    "            corelation = torch.FloatTensor(self.atn_dim, self.atn_dim) # c \n",
    "            nn.init.xavier_uniform_(corelation)\n",
    "            self.corelations.append(nn.Parameter(corelation))\n",
    "            \n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        heads = list()\n",
    "        for i in range(self.num_head):\n",
    "            x_transformed = self.linears[i](x) # H*W\n",
    "            alpha = self.attention_matrix(x_transformed, self.corelations[i], adj)\n",
    "            x_head = torch.matmul(alpha, x_transformed) # alpha * H*W\n",
    "            heads.append(x_head)\n",
    "        output = torch.cat(heads, dim=2)\n",
    "        return output\n",
    "            \n",
    "    def attention_matrix(self, x_transformed, corelation, adj):\n",
    "        \n",
    "        '''\n",
    "        x_transformed : H*W\n",
    "        \n",
    "        einsum\n",
    "        x_transformed는 3차원 [batch, n atom, features]이고 corelation은 [feature, feature]이다.\n",
    "        einsum을 통해 간단히 곱을 할 수 있음.\n",
    "        '''\n",
    "        x = torch.einsum('akj,ij->aki', (x_transformed, corelation))\n",
    "        alpha = torch.matmul(x, torch.transpose(x_transformed, 1, 2))\n",
    "        alpha = torch.mul(alpha, adj) # element wise product -> 0이 있는 부분들은 무시함.\n",
    "        alpha = self.tanh(alpha)\n",
    "        return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "front-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNBlock(nn.Module):\n",
    "    \n",
    "    \n",
    "    '''GCNLayer을 블락으로 만들어보자..\n",
    "    n_layer : layer를 몇개 쌓을 것이냐.\n",
    "    in_dim : \n",
    "    hidden_dim :\n",
    "    out_dim :\n",
    "    n_atom : # of max_atom\n",
    "    bn : batch_norm\n",
    "    sc : skip connection\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_layer, in_dim, hidden_dim, out_dim, n_atom, bn=True, atn=True, num_head=1, sc='gsc', dropout=0):\n",
    "        super(GCNBlock, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(n_layer):\n",
    "            self.layers.append(GCNLayer(in_dim if i==0 else hidden_dim,\n",
    "                                        out_dim if i==n_layer-1 else hidden_dim,\n",
    "                                        n_atom,\n",
    "                                        nn.ReLU() if i!=n_layer-1 else None,\n",
    "                                        bn,\n",
    "                                        atn,\n",
    "                                        num_head,\n",
    "                                        dropout))\n",
    "        self.relu = nn.ReLU()\n",
    "        if sc=='gsc':\n",
    "            self.sc = GatedSkipConnection(in_dim, out_dim)\n",
    "        elif sc=='sc':\n",
    "            self.sc = SkipConnection(in_dim, out_dim)\n",
    "        elif sc=='no':\n",
    "            self.sc = None\n",
    "        else:\n",
    "            assert False, \"Wrong sc type., sc should be 'gsc','sc', or 'no' \"\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        residual = x\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            out, adj = layer((x if i==0 else out), adj) # gcnlayer니까 아웃풋이 두개야\n",
    "        if self.sc != None:\n",
    "            out = self.sc(residual, out)\n",
    "        out = self.relu(out)\n",
    "        return out, adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "strategic-invite",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadOut(nn.Module):\n",
    "    '''permutation invariant를 만족하기 위한 파트'''\n",
    "    def __init__(self, in_dim, out_dim, act=None):\n",
    "        super(ReadOut, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim= out_dim\n",
    "        \n",
    "        self.linear = nn.Linear(self.in_dim, \n",
    "                                self.out_dim)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        self.activation = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        out = torch.sum(out, 1) # 1번째 차원을 없애는 쪽으로 더해줌. 즉 dense layer하나 통과하면 [batch, atom, feature]인데,이것을 atom이 없어지는 방향으로 더함.\n",
    "        if self.activation != None:\n",
    "            out = self.activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "changed-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    '''FFN'''\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, act=None):\n",
    "        super(Predictor, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.linear = nn.Linear(self.in_dim,\n",
    "                                self.out_dim)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        self.activation = act\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        if self.activation != None:\n",
    "            out = self.activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "killing-evening",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(GCNNet, self).__init__()\n",
    "        \n",
    "        self.blocks = nn.ModuleList() # 블럭들을 담을 통\n",
    "        \n",
    "        \n",
    "        for i in range(args.n_block):\n",
    "            # n_layer, in_dim, hidden_dim, out_dim, n_atom, bn=True, sc='gsc', act = None): \n",
    "            self.blocks.append(GCNBlock(args.n_layer,\n",
    "                                        args.in_dim if i==0 else args.hidden_dim,\n",
    "                                        args.hidden_dim,\n",
    "                                        args.hidden_dim,\n",
    "                                        args.n_atom,\n",
    "                                        args.bn,\n",
    "                                        args.atn,\n",
    "                                        args.num_head,\n",
    "                                        args.sc,\n",
    "                                        args.dropout))\n",
    "            \n",
    "        self.readout = ReadOut(args.hidden_dim, \n",
    "                               args.pred_dim1,\n",
    "                               act=nn.ReLU())\n",
    "        self.pred1 = Predictor(args.pred_dim1,\n",
    "                               args.pred_dim2,\n",
    "                               act=nn.ReLU())\n",
    "        self.pred2 = Predictor(args.pred_dim2,\n",
    "                               args.pred_dim3,\n",
    "                               act=nn.Tanh())\n",
    "        self.pred3 = Predictor(args.pred_dim3,\n",
    "                               args.out_dim) # 마지막에는 act안했는데, continuous한 값을 예측할 것이기 때문에..\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            out, adj = block((x if i==0 else out), adj)\n",
    "        out = self.readout(out)\n",
    "        out = self.pred1(out)\n",
    "        out = self.pred2(out)\n",
    "        out = self.pred3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-california",
   "metadata": {},
   "source": [
    "# 3. Train, Validate, Test and Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "sunset-cocktail",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, optimizer, criterion, data_train, bar, args):\n",
    "    epoch_train_loss = 0\n",
    "    for i, batch in enumerate(data_train):\n",
    "        list_feature = torch.tensor(batch[0]).to(device).float()\n",
    "        list_adj = torch.tensor(batch[1]).to(device).float()\n",
    "        list_logP = torch.tensor(batch[2]).to(device).float()\n",
    "        list_logP = list_logP.view(-1,1)\n",
    "                \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        list_pred_logP = model(list_feature, list_adj)\n",
    "        list_pred_logP.require_grad = False\n",
    "        train_loss = criterion(list_pred_logP, list_logP)\n",
    "        epoch_train_loss += train_loss.item()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "        bar.update(len(list_feature))\n",
    "\n",
    "    epoch_train_loss /= len(data_train)\n",
    "    \n",
    "    return model, epoch_train_loss\n",
    "\n",
    "def validate(model, device, criterion, data_val, bar, args):\n",
    "    epoch_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_val):\n",
    "            list_feature = torch.tensor(batch[0]).to(device).float()\n",
    "            list_adj = torch.tensor(batch[1]).to(device).float()\n",
    "            list_logP = torch.tensor(batch[2]).to(device).float()\n",
    "            list_logP = list_logP.view(-1,1)\n",
    "\n",
    "            model.eval()\n",
    "            list_pred_logP = model(list_feature, list_adj)\n",
    "            list_pred_logP.require_grad = False\n",
    "            val_loss = criterion(list_pred_logP, list_logP)\n",
    "            epoch_val_loss += val_loss.item()\n",
    "            \n",
    "            bar.update(len(list_feature))\n",
    "\n",
    "    epoch_val_loss /= len(data_val)\n",
    "    \n",
    "    return model, epoch_val_loss\\\n",
    "\n",
    "def test(model, device, data_test, args):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logP_total = list()\n",
    "        pred_logP_total = list()\n",
    "        for i, batch in enumerate(data_test):\n",
    "            list_feature = torch.tensor(batch[0]).to(device).float()\n",
    "            list_adj = torch.tensor(batch[1]).to(device).float()\n",
    "            list_logP = torch.tensor(batch[2]).to(device).float()\n",
    "            logP_total += list_logP.tolist()\n",
    "            list_logP = list_logP.view(-1,1)\n",
    "\n",
    "            list_pred_logP = model(list_feature, list_adj)\n",
    "            pred_logP_total += list_pred_logP.view(-1).tolist()\n",
    "\n",
    "        mae = mean_absolute_error(logP_total, pred_logP_total)\n",
    "        std = np.std(np.array(logP_total)-np.array(pred_logP_total))\n",
    "        \n",
    "    return mae, std, logP_total, pred_logP_total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "wooden-medicare",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(dict_partition, device, bar, args):\n",
    "    time_start = time.time()\n",
    "    \n",
    "    model = GCNNet(args)\n",
    "    model.to(device)\n",
    "        \n",
    "    if args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(),\n",
    "                               lr=args.lr,\n",
    "                               weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(),\n",
    "                               lr=args.lr,\n",
    "                               weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(),\n",
    "                               lr=args.lr,\n",
    "                               weight_decay=args.l2_coef)\n",
    "    else:\n",
    "        assert False, 'Undefined Optimizer Type'\n",
    "        \n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
    "                                          step_size=args.step_size,\n",
    "                                          gamma=args.gamma)\n",
    "\n",
    "    list_train_loss = list()\n",
    "    list_val_loss = list()\n",
    "\n",
    "    data_train = DataLoader(dict_partition['train'], \n",
    "                            batch_size=args.batch_size,\n",
    "                            shuffle=args.shuffle)\n",
    "\n",
    "    data_val = DataLoader(dict_partition['val'],\n",
    "                          batch_size=args.batch_size,\n",
    "                          shuffle=args.shuffle)\n",
    "\n",
    "    for epoch in range(args.epoch):\n",
    "        scheduler.step()\n",
    "        model, train_loss = train(model, device, optimizer, criterion, data_train, bar, args)\n",
    "        list_train_loss.append(train_loss)\n",
    "        \n",
    "        model, val_loss = validate(model, device, criterion, data_val, bar, args)\n",
    "        list_val_loss.append(val_loss)\n",
    "\n",
    "    data_test = DataLoader(dict_partition['test'],\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=args.shuffle)\n",
    "\n",
    "    mae, std, logP_total, pred_logP_total = test(model, device, data_test, args)\n",
    "        \n",
    "    time_end = time.time()\n",
    "    time_required = time_end - time_start\n",
    "    \n",
    "    args.list_train_loss = list_train_loss\n",
    "    args.list_val_loss = list_val_loss\n",
    "    args.logP_total = logP_total\n",
    "    args.pred_logP_total = pred_logP_total\n",
    "    args.mae = mae\n",
    "    args.std = std\n",
    "    args.time_required = time_required\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-document",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "indian-syndrome",
   "metadata": {},
   "source": [
    "# 5. visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "literary-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance(df_result, var1, var2):\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(10, 5)\n",
    "    \n",
    "    df_mae = df_result.pivot(var1, var2, 'mae')\n",
    "    df_std = df_result.pivot(var1, var2, 'std')\n",
    "    df_mae = df_mae[df_mae.columns].astype(float)\n",
    "    df_std = df_std[df_std.columns].astype(float)\n",
    "    \n",
    "    hm_mae = sns.heatmap(df_mae, ax=ax[0], annot=True, fmt='f', linewidths=.5, cmap='YlGnBu')\n",
    "    hm_std = sns.heatmap(df_std, ax=ax[1], annot=True, fmt='f', linewidths=.5, cmap='YlGnBu')\n",
    "    \n",
    "    fig.suptitle('Performance depends on ' + var1 + ' vs ' + var2)\n",
    "    hm_mae.set_title('MAE depends on ' + var1 + ' vs ' + var2)\n",
    "    hm_std.set_title('Std depends on ' + var1 + ' vs ' + var2)   \n",
    "    \n",
    "    \n",
    "def plot_performance_bar(df_result, var1, var2):\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(10, 5)\n",
    "    \n",
    "    sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"}) \n",
    "    bar_mae = sns.barplot(x=var1, y='mae', hue=var2, data=df_result, ax=ax[0])\n",
    "    bar_std = sns.barplot(x=var1, y='std', hue=var2, data=df_result, ax=ax[1])\n",
    "    \n",
    "    bar_mae.set_title('MAE depends on ' + var1 + ' vs ' + var2)\n",
    "    bar_std.set_title('Std depends on ' + var1 + ' vs ' + var2)\n",
    "    fig.suptitle('Performance depends on ' + var1 + ' vs ' + var2)\n",
    "\n",
    "    \n",
    "def plot_loss(df_result, var1, var2, ylim):\n",
    "    def plot(x, ylim=1.0, **kwargs):\n",
    "        plt.plot(x[0], **kwargs)\n",
    "        plt.ylim(0.0, ylim)\n",
    "    \n",
    "    sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "    g = sns.FacetGrid(df_result, row=var1, col=var2, margin_titles=True)\n",
    "    g.map(plot, 'list_train_loss', ylim=ylim, label='Train Loss')\n",
    "    g.map(plot, 'list_val_loss', ylim=ylim, color='r', label='Validation Loss')\n",
    "    g.fig.suptitle('Loss vs Epochs depends on ' + var1 + ' vs ' + var2, size=16)\n",
    "    g.fig.subplots_adjust(top=.9)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_distribution(df_result, var1, var2):\n",
    "    \n",
    "    def scatter(x, y, **kwargs):\n",
    "        plt.scatter(x[0], y[0], alpha=0.3, s=2) # 0을 붙인 이유는, 리스트를 넣었는데, [[]]로 되어서, 0을 부여하여 리스트를 갖고오게..\n",
    "    def identity(x, y, **kwargs): # abline을 그리자\n",
    "        plt.plot(x[0], x[0], alpha=0.4, color='black')\n",
    "    \n",
    "    sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"}) # 배경색을 무엇으로 할 것인가\n",
    "    g = sns.FacetGrid(df_result, row=var1, col=var2, margin_titles=True) # dataframe, row, col, 제목이 너무 딱 붙지 않게(True)\n",
    "    g.map(scatter, 'logP_total', 'pred_logP_total') # g.map -> mapping to function : scatter function.\n",
    "    g.map(identity, 'logP_total', 'logP_total')\n",
    "    g.fig.suptitle('Truth Distribution depends on ' + var1 + ' vs ' + var2, size=16)\n",
    "    g.fig.subplots_adjust(top=.9)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "independent-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size = 30\n",
    "args.lr = 0.0001\n",
    "args.l2_coef = 0\n",
    "args.optim = 'Adam'\n",
    "args.epoch = 30\n",
    "args.n_block = 2\n",
    "args.n_layer = 2 #\n",
    "args.n_atom = 50 #\n",
    "args.in_dim = 58 #\n",
    "args.hidden_dim = 64 #\n",
    "args.pred_dim1 = 256\n",
    "args.pred_dim2 = 128\n",
    "args.pred_dim3 = 128 \n",
    "args.out_dim = 1 #\n",
    "args.bn = True #\n",
    "args.sc = 'no' #\n",
    "args.atn = False #\n",
    "args.step_size = 10\n",
    "args.gamma = 0.1\n",
    "args.num_head = 1 #\n",
    "args.dropout=0.2 #\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "swedish-tonight",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/3516821513.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  bar = tqdm_notebook(total=n_iter, file=sys.stdout, position=0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a36860141d78443bb7caf198c72b8bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3240000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rainism/miniforge3/envs/tensorflow/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n",
      "/Users/rainism/miniforge3/envs/tensorflow/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:0.01/n_block:1 took 100seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n",
      "/Users/rainism/miniforge3/envs/tensorflow/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:0.01/n_block:2 took 115seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n",
      "/Users/rainism/miniforge3/envs/tensorflow/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:0.01/n_block:3 took 157seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n",
      "/Users/rainism/miniforge3/envs/tensorflow/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:0.001/n_block:1 took 80seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n",
      "/Users/rainism/miniforge3/envs/tensorflow/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:0.001/n_block:2 took 123seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n",
      "/Users/rainism/miniforge3/envs/tensorflow/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:0.001/n_block:3 took 158seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n",
      "/Users/rainism/miniforge3/envs/tensorflow/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:0.0001/n_block:1 took 81seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n",
      "/Users/rainism/miniforge3/envs/tensorflow/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:0.0001/n_block:2 took 120seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n",
      "/Users/rainism/miniforge3/envs/tensorflow/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:0.0001/n_block:3 took 161seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n",
      "/Users/rainism/miniforge3/envs/tensorflow/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:1e-05/n_block:1 took 81seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n",
      "/Users/rainism/miniforge3/envs/tensorflow/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:1e-05/n_block:2 took 120seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_feature = torch.tensor(batch[0]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_adj = torch.tensor(batch[1]).to(device).float()\n",
      "/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/227615566.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  list_logP = torch.tensor(batch[2]).to(device).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:1e-05/n_block:3 took 159seconds.\n"
     ]
    }
   ],
   "source": [
    "list_lr = [0.01, 0.001, 0.0001, 0.00001]\n",
    "list_n_block = [1, 2, 3]\n",
    "#list_lr = [0.001]\n",
    "#list_n_block = [2]\n",
    "var1 = \"lr\"\n",
    "var2 = \"n_block\"\n",
    "\n",
    "dict_result = dict()\n",
    "n_iter = len(list_n_block)*len(list_lr)*args.epoch*(len(dict_partition['train'])+len(dict_partition['val']))\n",
    "bar = tqdm_notebook(total=n_iter, file=sys.stdout, position=0)\n",
    "\n",
    "for lr in list_lr:\n",
    "    for n_block in list_n_block:\n",
    "        args.lr = lr\n",
    "        args.n_block = n_block\n",
    "        args.exp_name = var1+':'+str(lr)+'/'+var2+':'+str(n_block)\n",
    "        result = vars(experiment(dict_partition, device, bar, args))\n",
    "        print(args.exp_name + \" took \" + str(int(args.time_required)) + \"seconds.\")\n",
    "        dict_result[args.exp_name] = copy.deepcopy(result)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "bar.close()\n",
    "\n",
    "df_result = pd.DataFrame(dict_result).transpose()\n",
    "df_result.to_json('lr vs n_block 50000.JSON', orient='table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "floppy-detector",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "plot_loss() missing 1 required positional argument: 'ylim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5782/4251969793.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplot_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplot_performance_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplot_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplot_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: plot_loss() missing 1 required positional argument: 'ylim'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_result = pd.read_json('lr vs n_block 50000.JSON', orient='table')\n",
    "\n",
    "plot_performance(df_result, var1, var2)\n",
    "plot_performance_bar(df_result, var1, var2)\n",
    "plot_loss(df_result, var1, var2)\n",
    "plot_distribution(df_result, var1, var2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-crash",
   "metadata": {},
   "source": [
    "## Conclude 1\n",
    "Experiment 1. lr vs n_block \\\n",
    "In this experiement, the preformance of regression was measured by the __Mean Absolute Error (MAE)__ and the __Standard Deviation (Std)__, with respect to the different __learning rate (lr)__ and __number of Skip Connection Block (n_block)__ used in the each model.\n",
    "\n",
    "The learning rate was chose to be the first hyperparameters to be tuned because if we find the best learning rate, no matter what the model architecture is, we can use that learning rate to future exepriemnts. Thus, the number of block, which effetcs the model architecture the most, was chose to be the second.\n",
    "#### Variable Domain\n",
    "- __lr__ : [0.01, 0.001, 0.0001, 0.00001]\n",
    "- __n_block__ : [1, 2, 3]\n",
    "\n",
    "### Results and Discussion\n",
    "Each figure at the top represents:\n",
    "1. The MAE and std value by heatmap, which shows the exact value of MAE and std.\n",
    "2. The MAE and std value by bar plot, which shows the tendency of performance depending on the variables.\n",
    "3. Train loss and validation loss.\n",
    "4. The distribution true logP value and predicted logP value of each molecule\n",
    "\n",
    "depending on the two variables.\n",
    "\n",
    "#### Notable Results\n",
    "- Learning rate of 0.001 showed the best performance.\n",
    "- There was no notable tendency between the learning rate and the number of GCN blocks.\n",
    "- No overfitting occured, but the models did not leanred with learning rate of 0.01.\n",
    "- Time spent for training each model was not that different depending on the number of GCN blocks.\n",
    "\n",
    "#### Discussion\n",
    "- Even though there are other parameters that defines a model, such as n_layer, usage of (gated) skip connection, etc, I assumed that n_block is the most important model-defining parameter. Since learning rate of 0.001 showed the best result irrespective of n_block, I concluded that 0.001 is the optimal learning rate of all of my models.\n",
    "- Model learned nothing with learning rate of 0.01. This is because the step of opimizer is too big to reach a local minimum.\n",
    "- As n_block increases, time spent on training increased, but not quite \n",
    "\n",
    "After this experiment, the learning rate and the number of GCN blocks were fixed to 0.001 and 3, which shoed the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f7922c-96a3-4785-a7d1-2eef59f4c8ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318a7017-0168-40fd-996b-239bdde2c429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e103e30-2713-499b-adce-fe3627fb4f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43575f6a-1ef1-4d01-889f-14a0bdc49517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074ad8c3-7a8f-4230-909f-b561cb893772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ce267d-2724-48c8-943e-311cfbf1109f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcbb76a-d8d8-47c9-8dc1-3353055fd622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5883c181-9973-4ec8-8c01-39203ca21eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-japan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-parade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fleet-initial",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, partition, optimizer, criterion, args):\n",
    "    trainloader = torch.utils.data.DataLoader(partition['train'], \n",
    "                                              batch_size=args.train_batch_size, \n",
    "                                              shuffle=True, num_workers=2)\n",
    "    net.train()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        optimizer.zero_grad() # [21.01.05 오류 수정] 매 Epoch 마다 .zero_grad()가 실행되는 것을 매 iteration 마다 실행되도록 수정했습니다. \n",
    "\n",
    "        # get the inputs\n",
    "        list_feature, list_adj, list_logP = data\n",
    "        list_feature = list_feature#.cuda().float()\n",
    "        list_adj = list_adj#.cuda().float()\n",
    "        list_logP = list_logP.view(-1,1)#.cuda().float().view(-1, 1)  # 마지막은 1*n으로 해줘야한다.\n",
    "        \n",
    "        outputs = net(list_feature, list_adj)\n",
    "\n",
    "        loss = criterion(outputs, list_logP)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    return net, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "rubber-store",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, partition, criterion, args):\n",
    "    valloader = torch.utils.data.DataLoader(partition['val'], \n",
    "                                            batch_size=args.test_batch_size, \n",
    "                                            shuffle=False, num_workers=2)\n",
    "    net.eval()\n",
    "    val_loss = 0 \n",
    "    with torch.no_grad(): # 그래디언트를 계산하는 일이 없도록 해야한다.\n",
    "        for data in valloader:\n",
    "            list_feature, list_adj, list_logP = data\n",
    "            list_feature = list_feature.cuda().float()\n",
    "            list_adj = list_adj.cuda().float()\n",
    "            list_logP = list_logP.cuda().float().view(-1, 1)\n",
    "            \n",
    "            outputs = net(list_feature, list_adj)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        val_loss = val_loss / len(valloader)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "southeast-point",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, partition, args):\n",
    "    testloader = torch.utils.data.DataLoader(partition['test'], \n",
    "                                             batch_size=args.test_batch_size, \n",
    "                                             shuffle=False, num_workers=2)\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        logP_total = list() # 리스트로 받고\n",
    "        pred_logP_total = list() # 리스트로 받고 -> 맨밑에 mae 게산을 위한것.!\n",
    "        for data in testloader:\n",
    "            list_feature, list_adj, list_logP = data\n",
    "            list_feature = list_feature.cuda().float()\n",
    "            list_adj = list_adj.cuda().float()\n",
    "            list_logP = list_logP.cuda().float()\n",
    "            logP_total += list_logP.tolist()\n",
    "            list_logP = list_logP.view(-1, 1)\n",
    "            \n",
    "            outputs = net(list_feature, list_adj)\n",
    "            pred_logP_total += outputs.view(-1).tolist()\n",
    "\n",
    "        mae = mean_absolute_error(logP_total, pred_logP_total)\n",
    "        std = np.std(np.array(logP_total)-np.array(pred_logP_total))\n",
    "    \n",
    "    return mae, std, logP_total, pred_logP_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "aboriginal-drinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(partition, args):\n",
    "  \n",
    "    net = GCNNet(args)\n",
    "#     net.cuda()\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    if args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    else:\n",
    "        raise ValueError('In-valid optimizer choice')\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "        \n",
    "    for epoch in range(args.epoch):  # loop over the dataset multiple times\n",
    "        ts = time.time()\n",
    "        net, train_loss = train(net, partition, optimizer, criterion, args)\n",
    "        val_loss = validate(net, partition, criterion, args)\n",
    "        te = time.time()\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
    "        \n",
    "    mae, std, logP_total, pred_logP_total = test(net, partition, args)    \n",
    "    \n",
    "    result = {}\n",
    "    result['train_losses'] = train_losses\n",
    "    result['val_losses'] = val_losses\n",
    "    result['mae'] = mae\n",
    "    result['std'] = std\n",
    "    result['logP_total'] = logP_total\n",
    "    result['pred_logP_total'] = pred_logP_total\n",
    "    return vars(args), result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-rescue",
   "metadata": {},
   "source": [
    "# 4.Manage Experiment Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-dancing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def save_exp_result(setting, result):\n",
    "    exp_name = setting['exp_name']\n",
    "    del setting['epoch']\n",
    "    del setting['test_batch_size']\n",
    "    \n",
    "    hash_key = hashlib.shal(str(setting).encode()).hexdigest()[:6]\n",
    "    filename = f'ignore/results/{exp_name}-{hash_key}.json'\n",
    "    result.update(setting)\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dum(result, f)\n",
    "        \n",
    "def load_exp_result(exp_name):\n",
    "    dir_path = 'ignore/results'\n",
    "    filename = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) if '.json' in f]\n",
    "    list_result = []\n",
    "    for filename in filenames:\n",
    "        if exp_name in filename:\n",
    "            with open(join(dir_path, filename), 'r') as infile:\n",
    "                results = json.load(infile)\n",
    "                list_result.append(results)\n",
    "    df = pd.DataFrame(list_result) # .drop(columns=[])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-fifth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-variable",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-appointment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-slope",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-premium",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-exclusive",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-passenger",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "latest-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Graph(object):\n",
    "\n",
    "#     # Initialize the matrix\n",
    "#     def __init__(self, size):\n",
    "#         self.adjMatrix = []\n",
    "#         for i in range(size):\n",
    "#             self.adjMatrix.append([0 for i in range(size)])\n",
    "#         self.size = size\n",
    "\n",
    "#     # Add edges\n",
    "#     def add_edge(self, v1, v2):\n",
    "#         if v1 == v2:\n",
    "#             print(\"Same vertex %d and %d\" % (v1, v2))\n",
    "#         self.adjMatrix[v1][v2] = 1\n",
    "#         self.adjMatrix[v2][v1] = 1\n",
    "\n",
    "#     # Remove edges\n",
    "#     def remove_edge(self, v1, v2):\n",
    "#         if self.adjMatrix[v1][v2] == 0:\n",
    "#             print(\"No edge between %d and %d\" % (v1, v2))\n",
    "#             return\n",
    "#         self.adjMatrix[v1][v2] = 0\n",
    "#         self.adjMatrix[v2][v1] = 0\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.size\n",
    "\n",
    "#     # Print the matrix\n",
    "#     def print_matrix(self):\n",
    "#         for row in self.adjMatrix:\n",
    "#             for val in row:\n",
    "#                 print('{:4}'.format(val)),\n",
    "#             print\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     g = Graph(5)\n",
    "#     g.add_edge(0, 1)\n",
    "#     g.add_edge(0, 2)\n",
    "#     g.add_edge(1, 2)\n",
    "#     g.add_edge(2, 0)\n",
    "#     g.add_edge(2, 3)\n",
    "\n",
    "#     g.print_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-seller",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
